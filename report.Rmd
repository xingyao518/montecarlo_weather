---
title: "StatComp Project 2: Scottish weather"
author: "LI Xingyao (s2502246)"
output:
  html_document:
    number_sections: no
  pdf_document:
    number_sections: no
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---

```{r setup, include = FALSE}
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.

# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_bw())
suppressPackageStartupMessages(library(StatCompLab))

# To give the same random number sequence every time the document is knit:ed,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
```

```{r code=readLines("functions.R"), eval=FALSE, echo=FALSE}
# Do not change this code chunk
# Load function definitions
source("C:/Users/xy/Desktop/project02/functions.R")
```
# Introduction
This report aims to investigate the potential relationship between seasonal variations and precipitation levels in Scotland. Additionally, a predictive model for future precipitation will be developed. The study utilizes data gathered from eight weather stations situated in Scotland, encompassing the time ranging from January 1st, 1960, to December 31st, 2018. The primary step is to import the data.
```{r echo=TRUE}
data(ghcnd_stations, package = "StatCompLab")
data(ghcnd_values, package = "StatCompLab")
```

# 1 Seasonal variability
## 1.1 Seasonal variability
The goal for this section is to model daily precipitation data for eight weather stations in Scotland, which is more complex than temperature data due to the presence of both zeros and positive values. Plots are required for temperature and precipitation data to observe their behavior during a random year and determine if there are any seasonal effects. Then, a Monte Carlo permutation test is designed with the null hypothesis $H_0$: The rain fall distribution is the same in winter and summer, and the alternative hypothesis $H_1$: The winter and summer distributions have different expected values. T = |winter average - summer average| is the test statistic used. Additionally, separate p-values and Monte Carlo standard deviations are calculated for each station. Lastly, an approximate 95% confidence interval for the p-value is established when most observed counts are zero.
```{r include=FALSE}
library(tidyverse)

# Define a function to divide seasons into summer and winter
add_season <- function(as) {
  as$Season <- case_when(
    as$Month %in% c(1, 2, 3, 10, 11, 12) ~ "Winter",
    as$Month %in% c(4, 5, 6, 7, 8, 9) ~ "Summer",
  )
  as
}
# Add season to ghcnd
ghcnd <- left_join(ghcnd_values, ghcnd_stations, by = "ID")%>%
  add_season()
head(ghcnd)
```
```{r echo=TRUE}
# Seasonal variability for temperature 
ghcnd %>%
  filter(Element %in% c("TMIN", "TMAX")) %>%
  filter(Year == "2002") %>%
  group_by(Name, Element, Month, Season) %>%
  ggplot(aes(DecYear, Value, colour = Element)) +
  geom_point() +
  facet_wrap(~ Name)

# Seasonal variability for precipitation
ghcnd %>%
  filter(Element %in% c("PRCP")) %>%
  filter(Year == "2002") %>%
  group_by(Name, Element, Month, Season) %>%
  ggplot(aes(DecYear, Value, colour = Element)) +
  geom_point() +
  facet_wrap(~ Name)
```
<br>
For the purpose of demonstration, the year 2002 was chosen at random to display the temperature and precipitation data over one year. As depicted in the two figures above, there is an evident seasonal variation in temperature, with higher values in summer and lower in winter, indicating that winter is colder than summer. On the other hand, it is challenging to detect any seasonal pattern in precipitation based solely on the data collected from the eight stations. However, a slight increase in the winter precipitation of BENMORE can be observed.

```{r echo=FALSE}
# Add summer column to ghcnd_values
summer_months <- c(4, 5, 6, 7, 8, 9)
ghcnd_values <- ghcnd_values %>%
  mutate(Summer = ifelse(Month %in% summer_months, TRUE, FALSE)) %>%
  # The Name column has been added here for future use
  left_join(ghcnd_stations %>% select(ID, Name), by = "ID")


# Calculate the average summer PRCP for each station
summer_av <- ghcnd_values %>%
  filter(Element == "PRCP") %>%
  filter(Summer == TRUE) %>%
  group_by(ID) %>% 
  summarise(Value = mean(Value))

# Calculate the average winter PRCP for each station
winter_av <- ghcnd_values %>%
  filter(Element == "PRCP") %>%
  filter(Summer == FALSE) %>%
  group_by(ID) %>% 
  summarise(Value = mean(Value))

# Compute observed (true) test statistic 
obs_stat <- abs(summer_av$Value - winter_av$Value)


# A table with Name, p_value, and confidence interval for 8 stations
monte_carlo_test <- function(data, alpha=0.05, n=1000){
  count <- rep(0, 8)
  ci_lower <- vector("numeric", 8)
  ci_upper <- vector("numeric", 8)
  se <- rep(0, 8)
  
  for (i in 1:n){
    # Generate a random permutation of summer indicator variable
    summer_perm <- sample(data$Summer, length(data$Summer), replace = FALSE)
    # Apply the permutation to the data frame
    ghcnd_perm <- data %>%
      mutate(Summer = summer_perm) %>%
      group_by(ID)
    # Calculate the null distribution 
    null_dist <- ghcnd_perm %>%
      summarise(winter_avg = mean(Value[!Summer], na.rm = TRUE),
                summer_avg = mean(Value[Summer], na.rm = TRUE), .groups="drop") %>%
      # The difference in means between summer and winter temperatures
      mutate(diff = abs(summer_avg - winter_avg))
    count <- count + (null_dist$diff >= obs_stat)
  }
  p_values <- count/n
  
  # Compute the standard error
  se <- sqrt(p_values*(1-p_values)/n)
  z_alpha <- qnorm(1-alpha/2)
  # Compute the confidence interval
  for (j in 1:8){
    if(p_values[j] > 0) {
      ci_lower[j] <- p_values[j] - z_alpha*se[j]
      ci_upper[j] <- p_values[j] + z_alpha*se[j]
    } else {
      ci_lower[j] <- 0
      ci_upper[j] <- 1 - 0.025^(1/n)
    }
  }
  # Return a table with Name, p_value, and confidence interval
  result <- data.frame(Name = unique(ghcnd_values$Name), 
                       p_value = p_values, 
                       CI_lower = ci_lower, 
                       CI_upper = ci_upper)
  return(result)
}
p_values <- monte_carlo_test(data = ghcnd_values, alpha = 0.05, n = 1000)
p_values
```
Based on the results above, it was observed that the null hypothesis was rejected at the $5\%$ significance level for certain weather stations, indicating that the expected values for winter and summer distributions were different. However, for other weather stations, such as “EDINBURGH: ROYAL BOTANIC GARDE”, the null hypothesis was not rejected. In addition, the confidence intervals for the p-values were relatively wide, suggesting that there was significant uncertainty in the estimates.

In conclusion, the analysis indicates that there may be variations in the precipitation distribution between winter and summer for specific weather stations. Nonetheless, further investigation is needed to validate these findings and explore the potential causes for any detected disparities.


# 2 Spatial weather prediction
## 2.1 Estimation and prediction
In spatial weather prediction, a new variable called Value_sqrt_avg is constructed, which is the square root of the monthly averaged precipitation values. This is done because the precipitation values are highly skewed and the variance increases with the mean value, making it difficult to create a model with constant variance. By taking the square root of the monthly averages, this issue is alleviated and a constant-variance model becomes more plausible.

For estimation and prediction, models are defined and estimated for the square root of the monthly averaged precipitation values in Scotland. The basic model, $M_0$, uses a combination of "Longitude", "Latitude", "Elevation", and "DecYear" as independent variables to capture seasonal variability. Additional covariates $\cos(2 \pi kt)$ and $\sin(2\pi kt)$ that vary with the "DecYear" variable are added to $M_0$ to model seasonal variability in models $M_1$, $M_2$, $M_3$, and $M_4$.

To compare the models, various statistics such as the F-statistic, p-values for the F-statistic and individual regression coefficients, and the residual standard error were used. Here, the model estimation results for $M_0$ to $M_4$ are displayed below.
```{r echo=TRUE}
ghcnd_values_sqrt <- ghcnd_values %>%
  filter(Element == "PRCP") %>%
  group_by(ID, Year, Month) %>%
  summarise(Value_sqrt_avg = sqrt(mean(Value)), DecYear = mean(DecYear), .groups="drop") %>%
  left_join(ghcnd_stations %>% select(ID, Name, Longitude, Latitude, Elevation), by = "ID")
ghcnd_values_sqrt
```

```{r echo=TRUE}
# M0 model
M0 <- lm(Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear, data = ghcnd_values_sqrt)
print(summary(M0))
```
```{r echo=TRUE}
# M1 model
M1 <- lm(Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear +
           cos(2*pi*DecYear) + sin(2*pi*DecYear), data = ghcnd_values_sqrt)
print(summary(M1))
```
```{r echo=TRUE}
# M2 model
M2 <- lm(Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear +
           cos(2*pi*DecYear) + sin(2*pi*DecYear) +
           cos(4*pi*DecYear) + sin(4*pi*DecYear), data = ghcnd_values_sqrt)
print(summary(M2))
```
```{r echo=TRUE}
# M3 model
M3 <- lm(Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear +
           cos(2*pi*DecYear) + sin(2*pi*DecYear) +
           cos(4*pi*DecYear) + sin(4*pi*DecYear) +
           cos(6*pi*DecYear) + sin(6*pi*DecYear), data = ghcnd_values_sqrt)
print(summary(M3))
```
```{r echo=TRUE}
# M4 model
M4 <- lm(Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear +
           cos(2*pi*DecYear) + sin(2*pi*DecYear) +
           cos(4*pi*DecYear) + sin(4*pi*DecYear) +
           cos(6*pi*DecYear) + sin(6*pi*DecYear) +
           cos(8*pi*DecYear) + sin(8*pi*DecYear), data = ghcnd_values_sqrt)
print(summary(M4))
```

A higher F-statistic indicates a better fit to the data, while a lower p-value for the F-statistic indicates stronger evidence against the null hypothesis. Lower p-values for the individual regression coefficients also indicate a stronger relationship between the variable and the response variable. A lower residual standard error indicates a better fit to the data.

To be concluded, Model 4 had the best overall fit to the data, with the highest F-statistic, lowest p-value, and lowest residual standard error. Models 1 to 4 showed significant relationships between the response variable and the additional sine and cosine terms. On the other hand, Model 0, which had the fewest predictor variables, had higher p-values than the other models, indicating that it was a less good fit to the data.

## 2.2 Assessment: Station and season differences
To evaluate how well the model can predict precipitation at new locations, we employ a stratified cross-validation approach that groups data by weather station. For each station, we fit the model on all other stations and calculate the prediction scores on the held-out station's data. We then aggregate the scores for each station and compute the overall cross-validated average scores for each month of the year.

Two different score metrics, namely Squared Error (SE) and Dawid-Sebastiani (DS) scores, are used to assess the model's predictive performance. The SE score calculates the average squared difference between observed and predicted values, while the DS score is a probabilistic measure that evaluates the model's predicted probabilities against observed frequencies.
```{r include=FALSE}
##4
library(tidyverse)
library(StatCompLab)
# Define a function to fit the models
fit_models <- function(K, data){
  data <- data %>%
    mutate(Month = factor(Month, levels = 1:12, labels = month.abb))
  
  # Create the formula 
  if (K == 0) {
    f <- as.formula("Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear")
  } else if (K > 0 & K <= 4) {
    f <- as.formula(paste0("Value_sqrt_avg ~ Longitude + Latitude + Elevation + DecYear + ",
                           paste0("cos(2*pi*", 1:K, "*DecYear) + sin(2*pi*", 1:K, "*DecYear)", collapse = " + ")))
  } else {
    stop("Invalid value of K")
  }
  
  # Fit the model
  fit <- lm(f, data = data)
  return(fit)
}

# Cross Validation for stations
cv_station <- function(K, score_type){
  # create an empty data frame to store the results
  ave_score <- data.frame(Name = integer(),
                          Year = integer(),
                          Month = integer(),
                          pred_score = numeric())
  for (i in unique(ghcnd_values_sqrt$Name)){
    prcp_station <- subset(ghcnd_values_sqrt, Name == i)
    stats <- subset(ghcnd_values_sqrt, Name != i)
    # Fit the model
    fit <- fit_models(K, data = stats)
    # Subset data for the year and month
    for (j in unique(prcp_station$Year)){
      prcp_year <- prcp_station %>% filter(Year == j)
      for (k in unique(prcp_year$Month)){
        prcp_data <- prcp_year %>% filter(Month == k)
        # Predict precipitation using the model fit
        pred <- predict(fit, newdata = prcp_data, se.fit = TRUE)
        pred_score <- score_prediction(fit, pred, prcp_data, K, score_type)
        # Check if pred_score is not NA
        if(!is.na(pred_score)){    
          ave_score <- rbind(ave_score, data.frame(Name = i,
                                                   Year = j,
                                                   Month = k,
                                                   pred_score = pred_score))
        }
      }
    }
  }
  # Calculate the average score for each station and summarise
  Score <- ave_score %>%
    group_by(Name, Month) %>%
    summarise(Name=Name, Month=Month, pred_score = mean(pred_score, na.rm = TRUE), .groups = "drop") %>% 
    distinct()
  return(Score)
}

# Define a function to calculate the SE and DS scores 
score_prediction <- function(fit, pred, prcp_data, K, score_type){
  # Calculate residual variance
  residual_variance <- sum(fit$residuals^2)/fit$df.residual
  # Calculate standard deviation of prediction
  sd_pred <- sqrt(pred$se.fit^2 + residual_variance)
  # Calculate SE score
  if (score_type=="se"){
    score <- proper_score(type = "se", obs = prcp_data$Value_sqrt_avg,
                          mean = pred$fit)
  }
  # Calculate DS score
  else if (score_type=="ds"){
    score <- proper_score(type = "ds", obs = prcp_data$Value_sqrt_avg,
                          mean = pred$fit, sd = sd_pred)
  }
  return(score)
}


library(reshape2)
library(ggplot2)
# Run cv_station function to create ave_score object
score_M0_se <- cv_station(K = 0, score_type = "se")
score_M1_se <- cv_station(K = 1, score_type = "se")
score_M2_se <- cv_station(K = 2, score_type = "se")
score_M3_se <- cv_station(K = 3, score_type = "se")
score_M4_se <- cv_station(K = 4, score_type = "se")

# Calculate the average pred_score for each station across all months
mean_score_M0 <- score_M0_se %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score_M1 <- score_M1_se %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score_M2 <- score_M2_se %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score_M3 <- score_M3_se %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score_M4 <- score_M4_se %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

# Run cv_station function to create ave_score objects for each month
score_M0_ds <- cv_station(K = 0, score_type = "ds")
score_M1_ds <- cv_station(K = 1, score_type = "ds")
score_M2_ds <- cv_station(K = 2, score_type = "ds")
score_M3_ds <- cv_station(K = 3, score_type = "ds")
score_M4_ds <- cv_station(K = 4, score_type = "ds")

# Calculate the average pred_score for each station across all months
mean_score1_M0 <- score_M0_ds%>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score1_M1 <- score_M1_ds %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score1_M2 <- score_M2_ds %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score1_M3 <- score_M3_ds %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

mean_score1_M4 <- score_M4_ds %>%
  group_by(Name) %>%
  summarize(mean_pred_score = mean(pred_score, na.rm = TRUE)) %>%
  arrange(mean_pred_score)

```


```{r echo=TRUE}
df_se <- data.frame(
  Name = mean_score_M0$Name,
  scores_M0 = mean_score_M0$mean_pred_score,
  scores_M1 = mean_score_M1$mean_pred_score,
  scores_M2 = mean_score_M2$mean_pred_score,
  scores_M3 = mean_score_M3$mean_pred_score,
  scores_M4 = mean_score_M4$mean_pred_score
)

# reshape the data into long format
df_long_se <- reshape2::melt(df_se, id.vars = "Name", variable.name = "Model", value.name = "MeanScore")

# create a bar chart for se scores
ggplot(df_long_se, aes(x = Name, y = MeanScore, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Mean Score", fill = "Model") +
  ggtitle("Mean Scores by Name and Model (SE)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

```{r echo=TRUE}
df_ds <- data.frame(
  Name = mean_score1_M0$Name,
  scores_M0 = mean_score1_M0$mean_pred_score,
  scores_M1 = mean_score1_M1$mean_pred_score,
  scores_M2 = mean_score1_M2$mean_pred_score,
  scores_M3 = mean_score1_M3$mean_pred_score,
  scores_M4 = mean_score1_M4$mean_pred_score
)

# reshape the data into long format
df_long_ds <- reshape2::melt(df_ds, id.vars = "Name", variable.name = "Model", value.name = "MeanScore")

# create a bar chart for se scores
ggplot(df_long_ds, aes(x = Name, y = MeanScore, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Mean Score", fill = "Model") +
  ggtitle("Mean Scores by Name and Model (DS)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
<br>
The plots above reveal that the score assessments for predicting precipitation are not uniformly accurate for all weather stations. Specifically, the SE scores of the M0 model highlight that the "BENMORE: YOUNGER BOTANIC GARDE" station is more difficult to predict compared to other stations. Additionally, the DS scores suggest that the "BENMORE" station has distinct characteristics that are not captured by the model. Furthermore, our analysis demonstrates that the models M1 to M4 have lower scores than the M0 model for each station, indicating better fit. Despite the varying levels of difficulty across stations, the model's overall predictive performance for precipitation is satisfactory, as indicated by the cross-validated average SE and DS scores.

```{r echo=TRUE}
cv_station(K = 0, score_type = "se")
cv_station(K = 1, score_type = "se")
cv_station(K = 2, score_type = "se")
cv_station(K = 3, score_type = "se")
cv_station(K = 4, score_type = "se")
cv_station(K = 0, score_type = "ds")
cv_station(K = 1, score_type = "ds")
cv_station(K = 2, score_type = "ds")
cv_station(K = 3, score_type = "ds")
cv_station(K = 4, score_type = "ds")

```
<br>
Upon analysis, it was found that models M1 to M4 consistently showed better model fitting than M0 for each season, as evidenced by their lower scores. The Squared Error scores suggest that the models perform better during summer months, as lower scores were observed for this season. Conversely, the Dawid Sebastiani scores indicate better performance during winter, with lower scores observed for this season. These differences in performance across seasons may be attributed to variations in precipitation patterns that are more difficult for the model to accurately capture during the winter season.

In conclusion, our findings suggest that the model has a reasonable ability to predict precipitation at new locations, with variations in performance across weather stations and seasons.


## Function definitions

```{r code=readLines("functions.R"), eval=FALSE, echo=TRUE}
# Do not change this code chunk
```

